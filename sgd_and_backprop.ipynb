{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD and Backpropagation Visual Tutorial\n",
    "## A comprehensive guide with visual explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“š SGD and Backpropagation Visual Tutorial\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This notebook covers:\")\n",
    "print(\"1. Partial Derivatives Refresher\")\n",
    "print(\"2. Gradient Descent Visualization\")\n",
    "print(\"3. Simple Neural Network Forward Pass\")\n",
    "print(\"4. Backpropagation Step by Step\")\n",
    "print(\"5. SGD Implementation\")\n",
    "print(\"6. Complete Training Loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§® PART 1: PARTIAL DERIVATIVES REFRESHER\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a simple function f(x,y) = xÂ² + yÂ²\n",
    "def simple_function(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Create a grid of points\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = simple_function(X, Y)\n",
    "\n",
    "# Plot the function\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "ax1.set_title('f(x,y) = xÂ² + yÂ²')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_title('Contour Plot')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "# Gradient visualization\n",
    "ax3 = fig.add_subplot(133)\n",
    "# Calculate gradients (partial derivatives)\n",
    "dx = 2 * X  # âˆ‚f/âˆ‚x = 2x\n",
    "dy = 2 * Y  # âˆ‚f/âˆ‚y = 2y\n",
    "\n",
    "# Plot gradient field (every 10th point for clarity)\n",
    "skip = 10\n",
    "ax3.quiver(X[::skip, ::skip], Y[::skip, ::skip],\n",
    "           dx[::skip, ::skip], dy[::skip, ::skip],\n",
    "           angles='xy', scale_units='xy', scale=1, alpha=0.7)\n",
    "ax3.contour(X, Y, Z, levels=10, alpha=0.3)\n",
    "ax3.set_title('Gradient Field')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“– Partial Derivatives Explained:\n",
    "For $f(x,y) = x^2 + y^2$:\n",
    "â€¢ $âˆ‚f/âˆ‚x = 2x$ (derivative with respect to x, treating y as constant)\n",
    "â€¢ $âˆ‚f/âˆ‚y = 2y$ (derivative with respect to y, treating x as constant)\n",
    "â€¢ The gradient $âˆ‡f = (âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y) = (2x, 2y)$\n",
    "â€¢ Gradient points in direction of steepest increase\n",
    "â€¢ For minimization, we go in the OPPOSITE direction: $-âˆ‡f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ PART 2: GRADIENT DESCENT VISUALIZATION\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_demo():\n",
    "    # Starting point\n",
    "    x_start, y_start = 2.5, 2.0\n",
    "    learning_rate = 0.1\n",
    "    num_steps = 50\n",
    "\n",
    "    # Store path\n",
    "    path_x = [x_start]\n",
    "    path_y = [y_start]\n",
    "\n",
    "    x, y = x_start, y_start\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        # Calculate gradients\n",
    "        grad_x = 2 * x  # âˆ‚f/âˆ‚x\n",
    "        grad_y = 2 * y  # âˆ‚f/âˆ‚y\n",
    "\n",
    "        # Update parameters (move opposite to gradient)\n",
    "        x = x - learning_rate * grad_x\n",
    "        y = y - learning_rate * grad_y\n",
    "\n",
    "        path_x.append(x)\n",
    "        path_y.append(y)\n",
    "\n",
    "    return path_x, path_y\n",
    "\n",
    "# Run gradient descent\n",
    "path_x, path_y = gradient_descent_demo()\n",
    "\n",
    "# Visualize the path\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Contour with path\n",
    "x_grid = np.linspace(-3, 3, 100)\n",
    "y_grid = np.linspace(-3, 3, 100)\n",
    "X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n",
    "Z_grid = simple_function(X_grid, Y_grid)\n",
    "\n",
    "ax1.contour(X_grid, Y_grid, Z_grid, levels=20, alpha=0.6)\n",
    "ax1.plot(path_x, path_y, 'ro-', linewidth=2, markersize=4)\n",
    "ax1.plot(path_x[0], path_y[0], 'go', markersize=10, label='Start')\n",
    "ax1.plot(path_x[-1], path_y[-1], 'ro', markersize=10, label='End')\n",
    "ax1.set_title('Gradient Descent Path')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Function value over iterations\n",
    "function_values = [simple_function(x, y) for x, y in zip(path_x, path_y)]\n",
    "ax2.plot(function_values, 'b-', linewidth=2)\n",
    "ax2.set_title('Function Value vs Iteration')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('f(x,y)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Parameter values over time\n",
    "ax3.plot(path_x, 'r-', linewidth=2, label='x')\n",
    "ax3.plot(path_y, 'b-', linewidth=2, label='y')\n",
    "ax3.set_title('Parameter Values vs Iteration')\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Parameter Value')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Gradient magnitude\n",
    "grad_magnitudes = [np.sqrt((2*x)**2 + (2*y)**2) for x, y in zip(path_x, path_y)]\n",
    "ax4.plot(grad_magnitudes, 'g-', linewidth=2)\n",
    "ax4.set_title('Gradient Magnitude vs Iteration')\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('||âˆ‡f||')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ” Key Observations:\n",
    "â€¢ The algorithm converges to the minimum at (0,0)\n",
    "â€¢ Function value decreases monotonically\n",
    "â€¢ Parameters approach zero\n",
    "â€¢ Gradient magnitude decreases (getting closer to flat region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  PART 3: SIMPLE NEURAL NETWORK FORWARD PASS\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Network: 2 inputs -> 2 hidden -> 1 output\n",
    "        # Initialize weights randomly\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(2, 2) * 0.5  # Input to hidden\n",
    "        self.b1 = np.zeros((1, 2))             # Hidden bias\n",
    "        self.W2 = np.random.randn(2, 1) * 0.5  # Hidden to output\n",
    "        self.b2 = np.zeros((1, 1))             # Output bias\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Store intermediate values for backpropagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "# Create network and sample data\n",
    "nn = SimpleNeuralNetwork()\n",
    "X_sample = np.array([[0.5, 0.3]])\n",
    "y_sample = np.array([[0.8]])\n",
    "\n",
    "print(\"ðŸ—ï¸ Network Architecture:\")\n",
    "print(\"Input Layer: 2 neurons\")\n",
    "print(\"Hidden Layer: 2 neurons (sigmoid activation)\")\n",
    "print(\"Output Layer: 1 neuron (sigmoid activation)\")\n",
    "print(\"\\nInitial Weights:\")\n",
    "print(f\"W1 (input->hidden):\\n{nn.W1}\")\n",
    "print(f\"b1 (hidden bias): {nn.b1}\")\n",
    "print(f\"W2 (hidden->output):\\n{nn.W2}\")\n",
    "print(f\"b2 (output bias): {nn.b2}\")\n",
    "\n",
    "# Forward pass\n",
    "output = nn.forward(X_sample)\n",
    "print(f\"\\nForward Pass:\")\n",
    "print(f\"Input: {X_sample}\")\n",
    "print(f\"Hidden layer pre-activation (z1): {nn.z1}\")\n",
    "print(f\"Hidden layer post-activation (a1): {nn.a1}\")\n",
    "print(f\"Output layer pre-activation (z2): {nn.z2}\")\n",
    "print(f\"Final output (a2): {output}\")\n",
    "print(f\"Target: {y_sample}\")\n",
    "\n",
    "# Visualize the network\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Network diagram\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Input layer\n",
    "ax1.scatter([2, 2], [7, 3], s=200, c='lightblue', edgecolors='black')\n",
    "ax1.text(1.5, 7, 'xâ‚', fontsize=12, ha='center')\n",
    "ax1.text(1.5, 3, 'xâ‚‚', fontsize=12, ha='center')\n",
    "\n",
    "# Hidden layer\n",
    "ax1.scatter([5, 5], [7, 3], s=200, c='lightgreen', edgecolors='black')\n",
    "ax1.text(4.5, 7, 'hâ‚', fontsize=12, ha='center')\n",
    "ax1.text(4.5, 3, 'hâ‚‚', fontsize=12, ha='center')\n",
    "\n",
    "# Output layer\n",
    "ax1.scatter([8], [5], s=200, c='lightcoral', edgecolors='black')\n",
    "ax1.text(7.5, 5, 'y', fontsize=12, ha='center')\n",
    "\n",
    "# Connections\n",
    "connections = [\n",
    "    ((2, 7), (5, 7)), ((2, 7), (5, 3)),\n",
    "    ((2, 3), (5, 7)), ((2, 3), (5, 3)),\n",
    "    ((5, 7), (8, 5)), ((5, 3), (8, 5))\n",
    "]\n",
    "\n",
    "for start, end in connections:\n",
    "    ax1.plot([start[0], end[0]], [start[1], end[1]], 'k-', alpha=0.3)\n",
    "\n",
    "ax1.set_title('Network Architecture')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Activation visualization\n",
    "layers = ['Input', 'Hidden', 'Output']\n",
    "values = [X_sample[0], nn.a1[0], output[0]]\n",
    "\n",
    "for i, (layer, vals) in enumerate(zip(layers, values)):\n",
    "    if len(vals) == 1:\n",
    "        ax2.bar([i], vals, color=['lightblue', 'lightgreen', 'lightcoral'][i], alpha=0.7)\n",
    "        ax2.text(i, vals[0] + 0.02, f'{vals[0]:.3f}', ha='center')\n",
    "    else:\n",
    "        for j, val in enumerate(vals):\n",
    "            ax2.bar([i - 0.2 + j * 0.4], [val],\n",
    "                   color=['lightblue', 'lightgreen', 'lightcoral'][i],\n",
    "                   alpha=0.7, width=0.3)\n",
    "            ax2.text(i - 0.2 + j * 0.4, val + 0.02, f'{val:.3f}', ha='center')\n",
    "\n",
    "ax2.set_title('Activation Values')\n",
    "ax2.set_xticks(range(len(layers)))\n",
    "ax2.set_xticklabels(layers)\n",
    "ax2.set_ylabel('Activation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ PART 4: BACKPROPAGATION STEP BY STEP\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackpropNetwork(SimpleNeuralNetwork):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def backward(self, X, y, learning_rate=0.1):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Forward pass (already done, but for completeness)\n",
    "        output = self.forward(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = 0.5 * np.mean((y - output)**2)\n",
    "\n",
    "        # Backward pass\n",
    "        # Output layer gradients\n",
    "        dL_da2 = -(y - output) / m  # Derivative of loss w.r.t. output\n",
    "        da2_dz2 = self.sigmoid_derivative(self.a2)  # Derivative of sigmoid\n",
    "        dL_dz2 = dL_da2 * da2_dz2\n",
    "\n",
    "        # Gradients for W2 and b2\n",
    "        dL_dW2 = np.dot(self.a1.T, dL_dz2)\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "\n",
    "        # Hidden layer gradients\n",
    "        dL_da1 = np.dot(dL_dz2, self.W2.T)\n",
    "        da1_dz1 = self.sigmoid_derivative(self.a1)\n",
    "        dL_dz1 = dL_da1 * da1_dz1\n",
    "\n",
    "        # Gradients for W1 and b1\n",
    "        dL_dW1 = np.dot(X.T, dL_dz1)\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # Store gradients for visualization\n",
    "        self.gradients = {\n",
    "            'dL_dW2': dL_dW2, 'dL_db2': dL_db2,\n",
    "            'dL_dW1': dL_dW1, 'dL_db1': dL_db1,\n",
    "            'dL_da2': dL_da2, 'dL_dz2': dL_dz2,\n",
    "            'dL_da1': dL_da1, 'dL_dz1': dL_dz1\n",
    "        }\n",
    "\n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dL_dW2\n",
    "        self.b2 -= learning_rate * dL_db2\n",
    "        self.W1 -= learning_rate * dL_dW1\n",
    "        self.b1 -= learning_rate * dL_db1\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Create network and run one backprop step\n",
    "bp_nn = BackpropNetwork()\n",
    "loss = bp_nn.backward(X_sample, y_sample)\n",
    "\n",
    "print(\"ðŸ” Backpropagation Breakdown:\")\n",
    "print(f\"Loss: {loss:.6f}\")\n",
    "print(\"\\nGradients:\")\n",
    "for name, grad in bp_nn.gradients.items():\n",
    "    print(f\"{name}: {grad}\")\n",
    "\n",
    "# Visualize gradient flow\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Gradient magnitudes\n",
    "grad_names = ['dL_dW1', 'dL_db1', 'dL_dW2', 'dL_db2']\n",
    "grad_mags = [np.linalg.norm(bp_nn.gradients[name]) for name in grad_names]\n",
    "\n",
    "ax1.bar(grad_names, grad_mags, color=['lightblue', 'lightgreen', 'lightcoral', 'orange'])\n",
    "ax1.set_title('Gradient Magnitudes')\n",
    "ax1.set_ylabel('||âˆ‡||')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Weight updates visualization\n",
    "weight_names = ['W1', 'b1', 'W2', 'b2']\n",
    "weight_changes = [np.linalg.norm(0.1 * bp_nn.gradients[f'dL_{name}']) for name in weight_names]\n",
    "\n",
    "ax2.bar(weight_names, weight_changes, color=['lightblue', 'lightgreen', 'lightcoral', 'orange'])\n",
    "ax2.set_title('Weight Update Magnitudes')\n",
    "ax2.set_ylabel('||Î”weight||')\n",
    "\n",
    "# Loss landscape (simplified 2D slice)\n",
    "w_range = np.linspace(-2, 2, 50)\n",
    "losses = []\n",
    "original_w = bp_nn.W1[0, 0]\n",
    "\n",
    "for w in w_range:\n",
    "    bp_nn.W1[0, 0] = w\n",
    "    output = bp_nn.forward(X_sample)\n",
    "    loss = 0.5 * np.mean((y_sample - output)**2)\n",
    "    losses.append(loss)\n",
    "\n",
    "bp_nn.W1[0, 0] = original_w  # Restore original weight\n",
    "\n",
    "ax3.plot(w_range, losses, 'b-', linewidth=2)\n",
    "ax3.axvline(x=original_w, color='r', linestyle='--', label='Current weight')\n",
    "ax3.axvline(x=original_w - 0.1 * bp_nn.gradients['dL_dW1'][0, 0],\n",
    "           color='g', linestyle='--', label='After update')\n",
    "ax3.set_title('Loss Landscape (W1[0,0] slice)')\n",
    "ax3.set_xlabel('Weight Value')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Computation graph\n",
    "ax4.set_xlim(0, 10)\n",
    "ax4.set_ylim(0, 10)\n",
    "\n",
    "# Nodes\n",
    "nodes = [\n",
    "    (1, 8, 'X'), (3, 8, 'z1'), (5, 8, 'a1'), (7, 8, 'z2'), (9, 8, 'a2'),\n",
    "    (9, 6, 'L'), (7, 4, 'dL/dz2'), (5, 4, 'dL/da1'), (3, 4, 'dL/dz1'), (1, 4, 'dL/dX')\n",
    "]\n",
    "\n",
    "for x, y, label in nodes:\n",
    "    color = 'lightblue' if 'dL' in label else 'lightgreen'\n",
    "    ax4.scatter(x, y, s=400, c=color, edgecolors='black')\n",
    "    ax4.text(x, y, label, fontsize=10, ha='center', va='center')\n",
    "\n",
    "# Arrows\n",
    "arrows = [\n",
    "    ((1, 8), (3, 8)), ((3, 8), (5, 8)), ((5, 8), (7, 8)), ((7, 8), (9, 8)),\n",
    "    ((9, 8), (9, 6)), ((9, 6), (7, 4)), ((7, 4), (5, 4)), ((5, 4), (3, 4)), ((3, 4), (1, 4))\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    ax4.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='black'))\n",
    "\n",
    "ax4.set_title('Computation Graph')\n",
    "ax4.set_xticks([])\n",
    "ax4.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“ Key Backpropagation Concepts:\n",
    "â€¢ Chain Rule: $dL/dw = dL/da Ã— da/dz Ã— dz/dw$\n",
    "â€¢ Gradients flow backward through the network\n",
    "â€¢ Each layer's gradient depends on the gradient from the next layer\n",
    "â€¢ Weights are updated in the direction that reduces loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ² PART 5: STOCHASTIC GRADIENT DESCENT (SGD)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_data = np.random.randn(n_samples, 2)\n",
    "y_data = (X_data[:, 0] + X_data[:, 1] > 0).astype(int).reshape(-1, 1)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples\")\n",
    "print(f\"Input shape: {X_data.shape}\")\n",
    "print(f\"Output shape: {y_data.shape}\")\n",
    "\n",
    "# Visualize the dataset\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot of data\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_data.flatten()]\n",
    "ax1.scatter(X_data[:, 0], X_data[:, 1], c=colors, alpha=0.6)\n",
    "ax1.set_title('Dataset')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Class distribution\n",
    "unique, counts = np.unique(y_data, return_counts=True)\n",
    "ax2.bar(['Class 0', 'Class 1'], counts, color=['red', 'blue'], alpha=0.7)\n",
    "ax2.set_title('Class Distribution')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Batch GD vs SGD\n",
    "class SGDNetwork(BackpropNetwork):\n",
    "    def train_batch(self, X, y, epochs=100, learning_rate=0.1):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backward(X, y, learning_rate)\n",
    "            losses.append(loss)\n",
    "        return losses\n",
    "\n",
    "    def train_sgd(self, X, y, epochs=100, learning_rate=0.1, batch_size=1):\n",
    "        losses = []\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            epoch_losses = []\n",
    "\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "\n",
    "                loss = self.backward(X_batch, y_batch, learning_rate)\n",
    "                epoch_losses.append(loss)\n",
    "\n",
    "            losses.append(np.mean(epoch_losses))\n",
    "\n",
    "        return losses\n",
    "\n",
    "# Train networks\n",
    "sgd_net1 = SGDNetwork()  # Batch GD\n",
    "sgd_net2 = SGDNetwork()  # SGD\n",
    "\n",
    "batch_losses = sgd_net1.train_batch(X_data, y_data, epochs=50, learning_rate=0.1)\n",
    "sgd_losses = sgd_net2.train_sgd(X_data, y_data, epochs=50, learning_rate=0.1, batch_size=1)\n",
    "\n",
    "# Compare results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss comparison\n",
    "ax1.plot(batch_losses, label='Batch GD', linewidth=2)\n",
    "ax1.plot(sgd_losses, label='SGD', linewidth=2)\n",
    "ax1.set_title('Loss Comparison')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Smoothed SGD loss\n",
    "window_size = 5\n",
    "sgd_smoothed = np.convolve(sgd_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "ax2.plot(batch_losses, label='Batch GD', linewidth=2)\n",
    "ax2.plot(range(window_size-1, len(sgd_losses)), sgd_smoothed, label='SGD (smoothed)', linewidth=2)\n",
    "ax2.set_title('Smoothed Loss Comparison')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Final predictions comparison\n",
    "pred1 = sgd_net1.forward(X_data)\n",
    "pred2 = sgd_net2.forward(X_data)\n",
    "\n",
    "ax3.scatter(X_data[:, 0], X_data[:, 1], c=pred1.flatten(), cmap='RdYlBu', alpha=0.7)\n",
    "ax3.set_title('Batch GD Predictions')\n",
    "ax3.set_xlabel('Feature 1')\n",
    "ax3.set_ylabel('Feature 2')\n",
    "\n",
    "ax4.scatter(X_data[:, 0], X_data[:, 1], c=pred2.flatten(), cmap='RdYlBu', alpha=0.7)\n",
    "ax4.set_title('SGD Predictions')\n",
    "ax4.set_xlabel('Feature 1')\n",
    "ax4.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š SGD vs Batch GD Comparison:\n",
    "Final Batch GD Loss: {:.6f}\n".format(batch_losses[-1]),
    "Final SGD Loss: {:.6f}\n".format(sgd_losses[-1]),
    "Batch GD Accuracy: {:.3f}\n".format(np.mean((pred1 > 0.5) == y_data)),
    "SGD Accuracy: {:.3f}\n".format(np.mean((pred2 > 0.5) == y_data))
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ PART 6: COMPLETE TRAINING LOOP WITH MONITORING\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoredSGDNetwork(SGDNetwork):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.training_history = {\n",
    "            'loss': [], 'accuracy': [], 'weights': [], 'gradients': []\n",
    "        }\n",
    "\n",
    "    def train_with_monitoring(self, X, y, epochs=100, learning_rate=0.1, batch_size=10):\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            epoch_losses = []\n",
    "\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "\n",
    "                loss = self.backward(X_batch, y_batch, learning_rate)\n",
    "                epoch_losses.append(loss)\n",
    "\n",
    "            # Calculate metrics\n",
    "            predictions = self.forward(X)\n",
    "            accuracy = np.mean((predictions > 0.5) == y)\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "\n",
    "            # Store history\n",
    "            self.training_history['loss'].append(avg_loss)\n",
    "            self.training_history['accuracy'].append(accuracy)\n",
    "            self.training_history['weights'].append({\n",
    "                'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
    "                'W2': self.W2.copy(), 'b2': self.b2.copy()\n",
    "            })\n",
    "            # Note: Storing gradients directly in `training_history` for plotting later might require slight modification if not already available.\n",
    "            # For simplicity, if self.gradients is updated in backward, we can access it here.\n",
    "            # If self.gradients holds only the last batch's gradients, it won't be epoch-level.\n",
    "            # Assuming self.gradients holds the last calculated gradient for the purpose of the plot.\n",
    "            if hasattr(self, 'gradients'):\n",
    "                total_norm = 0\n",
    "                for name in ['dL_dW1', 'dL_db1', 'dL_dW2', 'dL_db2']:\n",
    "                    if name in self.gradients:\n",
    "                        total_norm += np.linalg.norm(self.gradients[name])**2\n",
    "                self.training_history['gradients'].append(np.sqrt(total_norm))\n",
    "            else:\n",
    "                 self.training_history['gradients'].append(0) # Placeholder if no gradient was calculated yet\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "# Train the monitored network\n",
    "monitored_net = MonitoredSGDNetwork()\n",
    "monitored_net.train_with_monitoring(X_data, y_data, epochs=100, learning_rate=0.5, batch_size=10)\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Loss and accuracy curves\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax1.plot(monitored_net.training_history['loss'], 'b-', linewidth=2)\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.plot(monitored_net.training_history['accuracy'], 'g-', linewidth=2)\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight evolution\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "W1_evolution = [w['W1'].flatten() for w in monitored_net.training_history['weights']]\n",
    "W1_evolution = np.array(W1_evolution)\n",
    "for i in range(W1_evolution.shape[1]):\n",
    "    ax3.plot(W1_evolution[:, i], alpha=0.7, label=f'W1[{i//2},{i%2}]')\n",
    "ax3.set_title('W1 Weight Evolution')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Weight Value')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Final decision boundary\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "predictions = monitored_net.forward(mesh_points)\n",
    "predictions = predictions.reshape(xx.shape)\n",
    "\n",
    "ax4.contourf(xx, yy, predictions, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "\n",
    "# Continue from ax4 decision boundary visualization\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_data.flatten()]\n",
    "ax4.scatter(X_data[:, 0], X_data[:, 1], c=colors, edgecolor='black', alpha=0.7)\n",
    "ax4.set_title('Final Decision Boundary')\n",
    "ax4.set_xlabel('Feature 1')\n",
    "ax4.set_ylabel('Feature 2')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm monitoring (optional, useful for stability)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.plot(monitored_net.training_history['gradients'], 'purple', linewidth=2)\n",
    "ax5.set_title('Gradient Norms (per epoch)')\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('Gradient L2 Norm')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Output histogram\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "final_outputs = monitored_net.forward(X_data)\n",
    "ax6.hist(final_outputs, bins=20, color='orange', alpha=0.7)\n",
    "ax6.set_title('Distribution of Final Outputs')\n",
    "ax6.set_xlabel('Output Value')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Training Complete\n",
    "***\n",
    "Final Loss: {:.6f}\n".format(monitored_net.training_history['loss'][-1]),
    "Final Accuracy: {:.3f}\n".format(monitored_net.training_history['accuracy'][-1]),
    "\n",
    "ðŸ§  You've completed a full walkthrough of:\n",
    "â€¢ Partial derivatives and gradients\n",
    "â€¢ Gradient descent dynamics\n",
    "â€¢ Forward and backward passes in a neural net\n",
    "â€¢ Batch vs stochastic optimization\n",
    "â€¢ Monitoring a full training loop\n",
    "\n",
    "Try adjusting hyperparameters or expanding the model next! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
