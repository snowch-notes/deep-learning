{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Detailed Tutorial: Softmax and Sparse Categorical Crossentropy\n",
        "\n",
        "In machine learning, particularly for classification tasks, it's crucial to understand the two key components that allow us to train a model: the **activation function** that gives us interpretable predictions, and the **loss function** that grades those predictions.\n",
        "\n",
        "This tutorial will walk you through their distinct roles using a practical, hard-coded example. We will explore:\n",
        "1.  **The Softmax Function:** How to convert raw model scores (logits) into probabilities.\n",
        "2.  **The Sparse Categorical Crossentropy Loss Function:** How to measure the error in those predictions.\n",
        "3.  **Two Ways to Calculate Loss:** The recommended `from_logits` method and the manual two-step method, so you can see exactly how they relate.\n",
        "\n",
        "> **NOTE:** Logits are the raw, unnormalized output scores from the final layer of a classification model before any activation function like softmax is applied.\n",
        ">\n",
        "> Think of them as the model's initial \"confidence scores\" for each possible class. These scores can be any real number (positive, negative, or zero). The softmax function then takes these logits and converts them into probabilities that sum to 1, making them much easier to interpret as the likelihood of each class."
      ],
      "metadata": {
        "id": "intro_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Scenario: A Mini-Classification Problem\n",
        "\n",
        "Let's imagine our model has just processed a batch of **2 samples**.\n",
        "\n",
        "Our task is to classify them into one of **4 possible classes** (indexed 0, 1, 2, 3)."
      ],
      "metadata": {
        "id": "scenario_cell"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# --- The Setup ---\n",
        "\n",
        "# The ground truth labels (the \"Answer Key\").\n",
        "# These are \"sparse\" because they are simple integers.\n",
        "# - The first sample belongs to class 2.\n",
        "# - The second sample belongs to class 1.\n",
        "y_true = tf.constant([2, 1])\n",
        "\n",
        "# The model's raw output (logits) for our 2 samples.\n",
        "# This is the fake output from a final Dense layer BEFORE any activation.\n",
        "# A higher number means the model is more confident.\n",
        "y_pred_logits = tf.constant([\n",
        "    # Sample 1: Model is most confident about class 0, but the true class is 2.\n",
        "    [2.0, 0.5, 1.0, 0.1],\n",
        "    # Sample 2: Model is most confident about class 1, which is correct.\n",
        "    [-1.0, 2.5, 0.2, 0.8]\n",
        "], dtype=tf.float32)\n",
        "\n",
        "print(\"--- Initial Data ---\")\n",
        "print(f\"True Labels (y_true): {y_true.numpy()}\")\n",
        "print(f\"Model's Raw Logits (y_pred_logits):\\n{y_pred_logits.numpy()}\")"
      ],
      "metadata": {
        "id": "setup_code_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e44a9b-e48a-4be6-d1d9-a5d3c2c3466f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Data ---\n",
            "True Labels (y_true): [2 1]\n",
            "Model's Raw Logits (y_pred_logits):\n",
            "[[ 2.   0.5  1.   0.1]\n",
            " [-1.   2.5  0.2  0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: The Softmax Function (The \"Converter\")\n",
        "\n",
        "The logits `[-1.0, 2.5, 0.2, 0.8]` are not easy to interpret. The role of the **softmax function** is to take these scores and convert them into a set of probabilities that are easy to understand and sum to 1. You can find more details about `tf.nn.softmax` in the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/nn/softmax).\n",
        "\n",
        "Let's apply `softmax` directly to our logits to see what the model's \"confidence\" looks like."
      ],
      "metadata": {
        "id": "part1_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Working with the Softmax Function ---\n",
        "\n",
        "# Apply softmax to convert logits to probabilities\n",
        "y_pred_probabilities = tf.nn.softmax(y_pred_logits)\n",
        "\n",
        "print(\"\\n--- Part 1: Softmax Output ---\")\n",
        "print(f\"Model Probabilities after Softmax:\\n{y_pred_probabilities.numpy()}\")\n",
        "\n",
        "# You can verify that each row sums to 1\n",
        "print(f\"Sum of probabilities for Sample 1:\"\n",
        "      f\" {np.sum(y_pred_probabilities.numpy()[0]):.2f}\")\n",
        "print(f\"Sum of probabilities for Sample 2:\"\n",
        "      f\" {np.sum(y_pred_probabilities.numpy()[1]):.2f}\")"
      ],
      "metadata": {
        "id": "part1_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbfd0f8-a992-45e1-9721-9f5f254c0233"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Part 1: Softmax Output ---\n",
            "Model Probabilities after Softmax:\n",
            "[[0.5745217  0.12819313 0.21135473 0.08593042]\n",
            " [0.02299632 0.7615336  0.07635049 0.13911964]]\n",
            "Sum of probabilities for Sample 1: 1.00\n",
            "Sum of probabilities for Sample 2: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** The output shows the model's confidence for each class. For the first sample, the model is now saying it's 57.5% sure the class is 0, 21.1% sure it's class 2, and so on. This is much more interpretable than the raw logits."
      ],
      "metadata": {
        "id": "part1_explanation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: The Loss Function (The \"Grader\")\n",
        "\n",
        "Now that we have predictions, we need to grade them. The **`SparseCategoricalCrossentropy` loss function** compares the model's predictions to the true labels (`y_true`) and calculates a penalty score. A lower score is better. You can find more details about `tf.keras.losses.SparseCategoricalCrossentropy` in the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy).\n",
        "\n",
        "There are two ways to do this, and seeing both makes the process clear."
      ],
      "metadata": {
        "id": "part2_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method A: The Recommended Way (`from_logits=True`)\n",
        "\n",
        "This is the standard, most numerically stable method. You give the raw logits directly to the loss function, and it handles the softmax conversion internally."
      ],
      "metadata": {
        "id": "part2a_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Working with the Loss Function (Method A) ---\n",
        "\n",
        "print(\"\\n--- Part 2A: Calculating Loss directly from Logits (Recommended) ---\")\n",
        "# Instantiate the loss function to accept raw logits\n",
        "scc_loss_fn_from_logits = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                          from_logits=True)\n",
        "\n",
        "# Calculate the loss for the entire batch using y_pred_logits\n",
        "loss_from_logits = scc_loss_fn_from_logits(y_true, y_pred_logits)\n",
        "\n",
        "print(f\"Keras Loss (from logits): {loss_from_logits.numpy()}\")"
      ],
      "metadata": {
        "id": "part2a_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb5f023-7219-42fe-b52b-b563b5b06fda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Part 2A: Calculating Loss directly from Logits (Recommended) ---\n",
            "Keras Loss (from logits): 0.9133191704750061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method B: The Explicit Two-Step Way\n",
        "\n",
        "To prove that the loss function uses softmax, we can do it in two steps: first, we apply softmax ourselves (as we did in Part 1), and then we feed these probabilities into the loss function. When we do this, we **must** tell the loss function that it's receiving probabilities, not logits, by setting `from_logits=False`."
      ],
      "metadata": {
        "id": "part2b_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Working with the Loss Function (Method B) ---\n",
        "\n",
        "print(\"\\n---Part 2B: Calculating Loss from Probabilities (Manual 2-Step) --\")\n",
        "# 1. We already have the probabilities from Part 1: y_pred_probabilities\n",
        "\n",
        "# 2. Instantiate the loss function to accept probabilities\n",
        "scc_loss_fn_from_probs = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                          from_logits=False)\n",
        "\n",
        "# 3. Calculate the loss from the probabilities we computed\n",
        "#    using y_pred_probabilities\n",
        "loss_from_probs = scc_loss_fn_from_probs(y_true, y_pred_probabilities)\n",
        "\n",
        "print(f\"Keras Loss (from probabilities): {loss_from_probs.numpy()}\")"
      ],
      "metadata": {
        "id": "part2b_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2d4d5a-b87a-43db-c1ef-a764efbdb3b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---Part 2B: Calculating Loss from Probabilities (Manual 2-Step) --\n",
            "Keras Loss (from probabilities): 0.9133191704750061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Notice the results from Method A and Method B are identical. This confirms that the loss function's primary job is to grade the probabilities that result from a softmax."
      ],
      "metadata": {
        "id": "part2_observation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: The Complete Manual Breakdown\n",
        "\n",
        "So how is that loss number actually calculated? Let's trace the calculation for the **first sample** manually, using the probabilities from Part 1.\n",
        "\n",
        "- **True Label:** `2`\n",
        "- **Model's Probabilities:** `[0.5745, 0.1281, 0.2113, 0.0859]`\n",
        "\n",
        "The loss function follows two simple steps:\n",
        "1.  It looks at the true label (`2`) and selects the model's predicted probability for **only that class**. In this case, it's the probability at index 2, which is `0.2113`.\n",
        "2.  It calculates the **negative natural logarithm** of that probability."
      ],
      "metadata": {
        "id": "part3_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Part 3: Manual Calculation for Sample 1 ---\")\n",
        "\n",
        "# The probabilities for the first sample\n",
        "probs_sample_1 = y_pred_probabilities[0]\n",
        "# The true label for the first sample\n",
        "label_sample_1 = y_true[0].numpy()\n",
        "\n",
        "# Step 1: Get the probability the model assigned to the CORRECT class (class 2)\n",
        "prob_for_correct_class = probs_sample_1[label_sample_1]\n",
        "print(f\"Probability for correct class ({label_sample_1}):\"\n",
        "      f\" {prob_for_correct_class.numpy():.4f}\")\n",
        "\n",
        "# Step 2: Calculate the cross-entropy loss for this single sample\n",
        "manual_loss_sample_1 = -tf.math.log(prob_for_correct_class)\n",
        "print(f\"Manual Loss for Sample 1\"\n",
        "      f\" [-log({prob_for_correct_class.numpy():.4f})]:\"\n",
        "      f\" {manual_loss_sample_1.numpy():.4f}\")"
      ],
      "metadata": {
        "id": "part3_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b7278f-2cf2-4c9d-d20b-fafc6e63901a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Part 3: Manual Calculation for Sample 1 ---\n",
            "Probability for correct class (2): 0.2114\n",
            "Manual Loss for Sample 1 [-log(0.2114)]: 1.5542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you were to do the same for the second sample (where loss will be low: `-log(0.887) = 0.119`), and then average the two results `(1.5543 + 0.119) / 2`, you would get the final loss value of `0.8369` that Keras calculated for the batch."
      ],
      "metadata": {
        "id": "part3_explanation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary: A Teacher-Student Analogy\n",
        "\n",
        "This entire process can be summarized with a simple analogy:\n",
        "\n",
        "- **The Model** is a student taking a multiple-choice test.\n",
        "- **Logits** are the student's raw, unorganized thoughts.\n",
        "- **Softmax (The Converter)** is the student being forced to commit to a final answer, showing their confidence for each option (e.g., \"I'm 21% sure it's C\").\n",
        "- **`y_true` (The Answer Key)** is the teacher's grading sheet.\n",
        "- **Crossentropy Loss (The Grader)** is the teacher calculating a \"disappointment score.\" They find the student's confidence for the *correct* answer and assign a penalty. Low confidence in the right answer gets a high penalty (high loss). High confidence gets a low penalty (low loss). This final score is what the student uses to learn."
      ],
      "metadata": {
        "id": "summary_cell"
      }
    }
  ]
}