{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Understanding Gradients with the Chain Rule\n",
    "\n",
    "This notebook explains the concept shown in the image: how to calculate the derivative of a loss function with respect to a model parameter using the **chain rule**. This is the fundamental operation behind how machine learning models, like neural networks, actually learn.\n",
    "\n",
    "We'll use a very simple linear model to make things clear:\n",
    "$$ \\text{Height} = \\text{Intercept} + (1 \\times \\text{Weight}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. The Setup: A Model and a Data Point\n",
    "\n",
    "First, let's set up the scenario. We have a single observed data point and a simple model. Our goal is to adjust the model's `Intercept` to make the model's prediction as close as possible to the observed data.\n",
    "\n",
    "- **Observed Data:** Let's say we have observed a person with `Weight = 3` kg and `Height = 7.5` cm. (We're using simple numbers, not realistic ones!)\n",
    "- **Model:** Our model predicts height based on weight. We'll fix the slope to `1` for simplicity, just like in the image. The only thing we can change is the `Intercept`.\n",
    "- **Prediction:** `Predicted_Height = (1 * Weight) + Intercept`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our single, observed data point\n",
    "observed_weight = 3\n",
    "observed_height = 7.5\n",
    "\n",
    "# Let's start with a random guess for the intercept\n",
    "initial_intercept = 2.0\n",
    "\n",
    "# The model's prediction function\n",
    "def predict(weight, intercept):\n",
    "    # Slope is fixed at 1\n",
    "    return 1 * weight + intercept\n",
    "\n",
    "# Get the model's initial prediction\n",
    "initial_prediction = predict(observed_weight, initial_intercept)\n",
    "\n",
    "print(f\"Observed Data Point: (Weight={observed_weight}, Height={observed_height})\")\n",
    "print(f\"Initial Intercept Guess: {initial_intercept}\")\n",
    "print(f\"Model's Initial Prediction for Height: {initial_prediction:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. The Error: Residual and Loss Function\n",
    "\n",
    "Our model's prediction (`5.0`) is not equal to the actual observed height (`7.5`). The difference between them is the error, which we call the **residual**.\n",
    "\n",
    "$$ \\text{Residual} = \\text{Observed} - \\text{Predicted} $$\n",
    "\n",
    "To get an overall \"loss\" or \"cost\", we square the residual. This does two things:\n",
    "1.  It makes the error positive, regardless of whether the prediction was too high or too low.\n",
    "2.  It penalizes larger errors more heavily.\n",
    "\n",
    "$$ \\text{Loss} = \\text{Residual}^2 = (\\text{Observed} - \\text{Predicted})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residual and the loss\n",
    "residual = observed_height - initial_prediction\n",
    "loss = residual**2\n",
    "\n",
    "print(f\"Residual (Error): {residual:.2f}\")\n",
    "print(f\"Squared Residual (Loss): {loss:.2f}\")\n",
    "\n",
    "# Let's visualize the prediction and the residual\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the observed point\n",
    "ax.plot(observed_weight, observed_height, 'ro', label='Observed Data', markersize=10)\n",
    "\n",
    "# Plot the model's prediction line\n",
    "weights = np.linspace(0, 5, 100)\n",
    "predictions = predict(weights, initial_intercept)\n",
    "ax.plot(weights, predictions, 'b-', label=f'Model (Intercept = {initial_intercept})')\n",
    "\n",
    "# Plot the prediction for our specific point\n",
    "ax.plot(observed_weight, initial_prediction, 'go', markersize=10, label='Predicted Height')\n",
    "\n",
    "# Draw the residual line\n",
    "ax.vlines(observed_weight, initial_prediction, observed_height, color='orange', linestyle='--', lw=2, label=f'Residual = {residual:.2f}')\n",
    "\n",
    "ax.set_xlabel('Weight')\n",
    "ax.set_ylabel('Height')\n",
    "ax.set_title('Model Prediction vs. Observed Data')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. The Goal: Minimizing Loss with the Chain Rule â›“ï¸\n",
    "\n",
    "Our goal is to change the `Intercept` to make the `Loss` as small as possible. Calculus tells us that the minimum of a function occurs where its derivative is zero.\n",
    "\n",
    "We need to find the derivative of the `Loss` with respect to the `Intercept`. This tells us how a small change in the `Intercept` will affect the `Loss`.\n",
    "\n",
    "$$ \\frac{d(\\text{Loss})}{d(\\text{Intercept})} = ? $$\n",
    "\n",
    "This is where the chain rule comes in, just as the image explains. We can't directly calculate this derivative easily, but we can see the *chain of influence*:\n",
    "\n",
    "1.  The `Intercept` influences the `Predicted` value.\n",
    "2.  The `Predicted` value influences the `Residual`.\n",
    "3.  The `Residual` influences the `Loss` (since `Loss = Residual^2`).\n",
    "\n",
    "The **Chain Rule** lets us break the problem down:\n",
    "\n",
    "$$ \\frac{d(\\text{Loss})}{d(\\text{Intercept})} = \\frac{d(\\text{Loss})}{d(\\text{Residual})} \\times \\frac{d(\\text{Residual})}{d(\\text{Intercept})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: Derivative of Loss w.r.t. Residual\n",
    "\n",
    "This part asks: If the `Residual` changes a little, how much does the `Loss` change? \n",
    "Since `Loss = Residual^2`:\n",
    "$$ \\frac{d(\\text{Loss})}{d(\\text{Residual})} = \\frac{d(\\text{Residual}^2)}{d(\\text{Residual})} = 2 \\times \\text{Residual} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Derivative of Residual w.r.t. Intercept\n",
    "This part asks: If the `Intercept` changes a little, how much does the `Residual` change?\n",
    "\n",
    "First, let's write `Residual` in terms of the `Intercept`:\n",
    "$$ \\text{Residual} = \\text{Observed} - \\text{Predicted} $$\n",
    "$$ \\text{Residual} = \\text{Observed} - (\\text{Weight} + \\text{Intercept}) $$\n",
    "$$ \\text{Residual} = \\text{Observed} - \\text{Weight} - \\text{Intercept} $$\n",
    "\n",
    "Now, let's take the derivative. Remember that `Observed` and `Weight` are just fixed numbers here.\n",
    "$$ \\frac{d(\\text{Residual})}{d(\\text{Intercept})} = \\frac{d(\\text{Observed} - \\text{Weight} - \\text{Intercept})}{d(\\text{Intercept})} = -1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Combination\n",
    "\n",
    "Now we just multiply the two parts together!\n",
    "\n",
    "$$ \\frac{d(\\text{Loss})}{d(\\text{Intercept})} = (2 \\times \\text{Residual}) \\times (-1) = -2 \\times \\text{Residual} $$\n",
    "\n",
    "This final expression is the **gradient**. It tells us the slope of the loss function with respect to the intercept. We can use it to update our intercept and make our model better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the gradient using our numbers\n",
    "\n",
    "# Part A\n",
    "dLoss_dResidual = 2 * residual\n",
    "print(f\"d(Loss)/d(Residual) = {dLoss_dResidual:.2f}\")\n",
    "\n",
    "# Part B\n",
    "dResidual_dIntercept = -1\n",
    "print(f\"d(Residual)/d(Intercept) = {dResidual_dIntercept}\")\n",
    "\n",
    "# Final Gradient\n",
    "gradient = dLoss_dResidual * dResidual_dIntercept\n",
    "print(f\"\\nFINAL GRADIENT: d(Loss)/d(Intercept) = {gradient:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Using the Gradient to Learn âœ…\n",
    "\n",
    "The gradient we calculated (`-5.0`) is the slope of the loss curve at our current `Intercept` of `2.0`. \n",
    "\n",
    "- A **negative gradient** means we need to **increase** the `Intercept` to move towards the minimum loss.\n",
    "- A **positive gradient** means we need to **decrease** the `Intercept`.\n",
    "\n",
    "This gives us the update rule for **gradient descent**:\n",
    "\n",
    "$$ \\text{new_intercept} = \\text{old_intercept} - (\\text{learning_rate} \\times \\text{gradient}) $$\n",
    "\n",
    "The `learning_rate` is a small number (e.g., 0.1) that controls how big of a step we take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "print(f\"Old Intercept: {initial_intercept:.2f}\")\n",
    "\n",
    "# Update the intercept using the gradient\n",
    "new_intercept = initial_intercept - (learning_rate * gradient)\n",
    "\n",
    "print(f\"New Intercept after one step: {new_intercept:.2f}\")\n",
    "\n",
    "# Let's see the new prediction and loss\n",
    "new_prediction = predict(observed_weight, new_intercept)\n",
    "new_residual = observed_height - new_prediction\n",
    "new_loss = new_residual**2\n",
    "\n",
    "print(f\"\\nNew Prediction: {new_prediction:.2f}\")\n",
    "print(f\"Old Loss: {loss:.2f} -> New Loss: {new_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! âœ¨ By taking one small step in the direction opposite to the gradient, we updated our `Intercept` from `2.0` to `2.5` and our `Loss` dropped significantly from `6.25` to `4.00`. \n",
    "\n",
    "Repeating this process many times is how machine learning models \"learn\" to find the best parameters that minimize the overall loss across all data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
