{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Calculus (for Machine Learning and Beyond)\n",
    "## Chapters 1 & 2 (Expanded Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Overview and Motivation\n",
    "\n",
    "Firstly, where does matrix calculus fit into the MIT course catalog? Well, there are 18.01 (Single-Variable Calculus) and 18.02 (Vector Calculus) that students are required to take at MIT. But it seems as though this sequence of material is being cut off arbitrarily:\n",
    "\n",
    "Scalar → Vector → Matrices → Higher-Order Arrays?\n",
    "\n",
    "After all, this is how the sequence is portrayed in many computer programming languages, including Julia! Why should calculus stop with vectors?\n",
    "\n",
    "In the last decade, linear algebra has taken on larger and larger importance in numerous areas, such as machine learning, statistics, engineering, etc. In this sense, linear algebra has gradually taken over a much larger part of today's tools for lots of areas of study; now everybody needs linear algebra. So it makes sense that we would want to do calculus on these higher-order arrays, and it won't be a simple/obvious generalization (for instance, $\\frac{d}{dA}A^{2}\\ne2A$ for non-scalar matrices A). More generally, the subjects of differentiation and sensitivity analysis are much deeper than one might suspect from the simple rules learned in first- or second-semester calculus. Differentiating functions whose inputs and/or outputs are in more complicated vector spaces (e.g. matrices, functions, or more) is one part of this subject. Another topic is the efficient evaluation of derivatives of functions involving very complicated calculations, from neural networks to huge engineering simulations; this leads to the topic of \"adjoint\" or \"reverse-mode\" differentiation, also known as \"backpropagation.\" Automatic differentiation (AD) of computer programs by compilers is another surprising topic, in which the computer does something very different from the typical human process of first writing out an explicit symbolic formula and then passing the chain rule through it. These are only a few examples: the key point is that differentiation is more complicated than you may realize, and that these complexities are increasingly relevant for a wide variety of applications. Let's quickly talk about some of these applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Applications\n",
    "\n",
    "**Applications: Machine learning**\n",
    "\n",
    "Machine learning has numerous buzzwords associated with it, including but not limited to: parameter optimization, stochastic gradient descent, automatic differentiation, and backpropagation. In this whole collage you can see a fraction of how matrix calculus applies to machine learning. It is recommended that you look into some of these topics yourself if you are interested.\n",
    "\n",
    "**Applications: Physical modeling**\n",
    "\n",
    "Large physical simulations, such as engineering-design problems, are increasingly characterized by huge numbers of parameters, and the derivatives of simulation outputs with respect to these parameters is crucial in order to evaluate sensitivity to uncertainties as well as to apply large-scale optimization. For example, the shape of an airplane wing might be characterized by thousands of parameters, and if you can compute the derivative of the drag force (from a large fluid-flow simulation) with respect to these parameters then you could optimize the wing shape to minimize the drag for a given lift or other constraints. An extreme version of such parameterization is known as \"topology optimization,\" in which the material at \"every point\" in space is potentially a degree of freedom, and optimizing over these parameters can discover not only a optimal shape but an optimal topology (how materials are connected in space, e.g. how many holes are present). For example, topology optimization has been applied in mechanical engineering to design the cross sections of airplane wings, artificial hips, and more into a complicated lattice of metal struts (e.g. minimizing weight for a given strength). Besides engineering design, complicated differentiation problems arise in fitting unknown parameters of a model to experimental data, and also in evaluating uncertainties in the outputs of models with imprecise parameters/inputs. This is closely related to regression problems in statistics, as discussed below, except that here the model might be a giant set of differential equations with some unknown parameters.\n",
    "\n",
    "**Applications: Data science and multivariable statistics**\n",
    "\n",
    "In multivariate statistics, models are often framed in terms of matrix inputs and outputs (or even more complicated objects such as tensors). For example, a \"simple\" linear multivariate matrix model might be $Y(X)=XB+U$, where B is an unknown matrix of coefficients (to be determined by some form of fit/regression) and U is an unknown matrix of random noise (that prevents the model from exactly fitting the data). Regression then involves minimizing some function of the error $U(B)=Y-XB$ between the model XB and data Y; for example, a matrix norm $||U||_{F}^{2}=tr~U^{T}U$ a determinant det $U^{T}U$ or more complicated functions. Estimating the best-fit coefficients B, analyzing uncertainties, and many other statistical analyses require differentiating such functions with respect to B or other parameters. A recent review article on this topic is Liu et al. (2022): \"Matrix differential calculus with applications in the multivariate linear model and its diagnostics\" (https://doi.org/10.1016/j.sctalk.2023.100274).\n",
    "\n",
    "**Applications: Automatic differentiation**\n",
    "\n",
    "Typical differential calculus classes are based on symbolic calculus, with students essentially learning to do what Mathematica or Wolfram Alpha can do. Even if you are using a computer to take derivatives symbolically, to use this effectively you need to understand what is going on beneath the hood. But while, similarly, some numerics may show up for a small portion of this class (such as to approximate a derivative using the difference quotient), today's automatic differentiation is neither of those two things. It is more in the field of the computer science topic of compiler technology than mathematics. However, the underlying mathematics of automatic differentiation is interesting, and we will learn about this in this class! Even approximate computer differentiation is more complicated than you might expect. For single-variable functions $f(x)$, derivatives are defined as the limit of a difference $[f(x+\\delta x)-f(x)]/\\delta x$ as $\\delta x\\rightarrow0$ A crude \"finite-difference\" approximation is simply to approximate $f^{\\prime}(x)$ by this formula for a small $\\delta x$ but this turns out to raise many interesting issues involving balancing truncation and roundoff errors, higher-order approximations, and numerical extrapolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 First Derivatives\n",
    "\n",
    "The derivative of a function of one variable is itself a function of one variable it simply is (roughly) defined as the linearization of a function. I.e., it is of the form $(f(x)-f(x_{0}))\\approx f^{\\prime}(x_{0})(x-x_{0})$. This formula tells us that the change in the function's output (the left side) is approximately equal to the derivative at a point $x_0$ multiplied by the change in the function's input $(x-x_0)$. In this sense, \"everything is easy\" with scalar functions of scalars (by which we mean, functions that take in one number and spit out one number).\n",
    "\n",
    "There are occasionally other notations used for this linearization:\n",
    "\n",
    "- $\\Delta y \\approx f'(x) \\Delta x$\n",
    "- $dy = f'(x)dx$\n",
    "- $(y-y_0) \\approx f'(x_0)(x-x_0)$\n",
    "- $df = f'(x)dx$\n",
    "\n",
    "This last one, the **differential notation**, will be the preferred of the above for this class. One can think of $dx$ and $dy$ as \"really small numbers.\" In mathematics, they are called infinitesimals, defined rigorously via taking limits. Note that here we do not want to divide by $dx$. While this is completely fine to do with scalars, once we get to vectors and matrices you can't always divide!\n",
    "\n",
    "The numerics of such derivatives are simple enough to play around with. For instance, consider the function $f(x)=x^{2}$ and the point $(x_{0},f(x_{0}))=(3,9)$. Then, we have the following numerical values near (3,9):\n",
    "\n",
    "$f(3.0001)=9.00060001$\n",
    "$f(3.00001)=9.0000600001$\n",
    "$f(3.000001)=9.00000600001$\n",
    "$f(3.0000001)=9.000006000001$\n",
    "\n",
    "Here, the bolded digits on the left are $\\Delta x$ and the bolded digits on the right are $\\Delta y$. Notice that the change in y is consistently 6 times the change in x. For instance, for the first line: $\\Delta y = 9.00060001 - 9 = 0.00060001$ and $\\Delta x = 3.0001 - 3 = 0.0001$. The ratio is $\\Delta y / \\Delta x \\approx 0.0006 / 0.0001 = 6$. This is because the derivative of $x^2$ is $2x$, and at $x=3$, the derivative is $2(3)=6$.\n",
    "\n",
    "Notice that $\\Delta y \\approx 6\\Delta x$. Hence, we have that\n",
    "\n",
    "$f(3 + \\Delta x) \\approx 9 + \\Delta y = 9+6\\Delta x$\n",
    "$f(3+\\Delta x)-f(3) \\approx 6\\Delta x \\approx f'(3)\\Delta x$.\n",
    "\n",
    "Therefore, we have that the linearization of $x^{2}$ at $x=3$ is the function $f(x)-f(3)\\approx6(x-3).$\n",
    "\n",
    "We now leave the world of scalar calculus and enter the world of vector/matrix calculus! Professor Edelman invites us to think about matrices **holistically**; not just as a table of numbers. This means we should consider the matrix as a single entity with its own properties, rather than just a collection of individual numbers. The notion of linearizing your function will conceptually carry over as we define the derivative of functions which take in/spit out more than one number. Of course, this means that the derivative will have a different \"shape\" than a single number. Here is a table on the shape of the first derivative. The inputs of the function are given on the left hand side of the table, and the outputs of the function are given across the top.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| input and output → | scalar | vector | matrix |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **scalar** | scalar | vector (for instance, velocity) | matrix |\n",
    "| **vector** | (column) vector gradient | matrix (called the Jacobian matrix) | higher order array |\n",
    "| **matrix** | matrix | higher order array | higher order array |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will ultimately learn how to do any of these in great detail eventually in this class! The purpose of this table is to plant the notion of differentials as linearization. Let's look at an example.\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "Let $f(x)=x^{T}x$, where x is a $2\\times1$ matrix (a column vector) and the output is thus a $1\\times1$ matrix (a scalar). Confirm that $2x_{0}^{T}dx$ is indeed the differential of f at $x_{0}=\\begin{pmatrix}3 \\\\ 4\\end{pmatrix}$ which means $x_0^T = \\begin{pmatrix} 3 & 4 \\end{pmatrix}$.\n",
    "\n",
    "Firstly, let's compute $f(x_{0})$:\n",
    "\n",
    "$f(x_{0})=x_{0}^{T}x_{0}= \\begin{pmatrix} 3 & 4 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = 3 \\cdot 3 + 4 \\cdot 4 = 3^{2}+4^{2}=25.$\n",
    "\n",
    "Then, suppose we have a small change in x, which we call $dx=\\begin{pmatrix} .001 \\\\ .002 \\end{pmatrix}$ Then, we would have that\n",
    "\n",
    "$x_0+dx = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} .001 \\\\ .002 \\end{pmatrix} = \\begin{pmatrix} 3.001 \\\\ 4.002 \\end{pmatrix}$\n",
    "\n",
    "$f(x_0+dx)=(3.001)^{2}+(4.002)^{2}=9.006001 + 16.016004 = 25.022005.$\n",
    "\n",
    "The change in f is $\\Delta f = f(x_0+dx) - f(x_0) = 25.022005 - 25 = 0.022005$.\n",
    "\n",
    "Then, notice that the proposed differential is $2x_{0}^{T}dx$. Let's compute this:\n",
    "\n",
    "$2x_{0}^{T}dx=2 \\begin{pmatrix} 3 & 4 \\end{pmatrix} \\begin{pmatrix} .001 \\\\ .002 \\end{pmatrix} = 2(3 \\cdot .001 + 4 \\cdot .002) = 2(0.003 + 0.008) = 2(0.011) = .022.$\n",
    "\n",
    "Hence, we have that\n",
    "\n",
    "$f(x_{0}+dx)-f(x_{0}) \\approx 2x_{0}^{T}dx=.022.$\n",
    "\n",
    "As you can see, the calculated change in f (0.022005) is very close to the value predicted by the differential (0.022). As we will see right now, the $2x_{0}^{T}dx$ didn't come from nowhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Intro: Matrix and Vector Product Rule\n",
    "\n",
    "For matrices, we in fact still have a product rule! We will discuss this in much more detail in later chapters, but let's begin here with a small taste.\n",
    "\n",
    "**Theorem 2 (Differential Product Rule)**\n",
    "\n",
    "Let A, B be two matrices. Then, we have the differential product rule for AB:\n",
    "\n",
    "$d(AB)=(dA)B+A(dB).$\n",
    "\n",
    "By the differential of the matrix A, we think of it as a small (unconstrained) change in the matrix A. Later, constraints may be places on the allowed perturbations. Notice however, that (by our table) the derivative of a matrix is a matrix! So generally speaking, the products will not commute, meaning $A(dB)$ is not the same as $(dB)A$.\n",
    "\n",
    "If x is a vector, then by the differential product rule we have\n",
    "\n",
    "$d(x^{T}x)=(dx^{T})x+x^{T}(dx)$.\n",
    "\n",
    "However, notice that this is a dot product. The term $(dx^T)x$ is a scalar (a 1x1 matrix), and so is $x^T(dx)$. Since dot products commute (because $\\sum a_{i}\\cdot b_{i}=\\sum b_{i}\\cdot a_{i})$, we have that $(dx^T)x = (x^T(dx))^T = (dx)^T(x^T)^T = (dx)^T x$. So the two terms are identical scalars. Therefore, we can combine them:\n",
    "$d(x^{T}x)=(dx^{T})x + (dx^T)x = 2(dx^T)x = 2x^Tdx$. We usually write the row vector $2x^T$ on the left. The result is equivalent to $(2x)^{T}dx$.\n",
    "\n",
    "**Remark 3.** The way the product rule works for vectors as matrices is that transposes \"go for the ride.\" See the next example below.\n",
    "\n",
    "**Example 4**\n",
    "\n",
    "By the product rule. we have\n",
    "\n",
    "1.  $d(u^{T}v)=(du)^{T}v+u^{T}(dv)$. Since $u^Tv$ is a scalar (dot product), its differential must also be a scalar. We know that for any scalar $s$, $s=s^T$. Thus, we can transpose the first term: $(du)^Tv = ((du)^Tv)^T = v^T((du)^T)^T = v^Tdu$. Since dot products commute, we can write the result as $v^{T}du+u^{T}dv$\n",
    "2.  $d(uv^{T})=(du)v^{T}+u(dv)^{T}.$ Here, $uv^T$ is a matrix (an outer product), not a scalar, so we cannot commute the terms.\n",
    "\n",
    "**Remark 5.** The way to prove these sorts of statements can be seen in Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Derivatives as Linear Operators\n",
    "\n",
    "We are now going to revisit the notion of a derivative in a way that we can generalize to higher-order arrays and other vector spaces. We will get into more detail on differentiation as a linear operator, and in particular, will dive deeper into some of the facts we have stated thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Revisiting single-variable calculus\n",
    "\n",
    "In a first-semester single-variable calculus course (like 18.01 at MIT), the derivative $f^{\\prime}(x)$ is introduced as the slope of the tangent line at the point $(x,f(x))$ which can also be viewed as a linear approximation of f near x. In particular, as depicted in Fig. 1, this is equivalent to a prediction of the change $\\delta f$ in the \"output\" of $f(x)$ from a small change $\\delta x$ in the \"input\" to first order (linear) in $\\delta x$:\n",
    "\n",
    "$\\delta f=f(x+\\delta x)-f(x)=f^{\\prime}(x)\\delta x+\\frac{\\text{(higher-order terms)}}{o(\\delta x)}$\n",
    "\n",
    "We can more precisely express these higher-order terms using asymptotic **\"little-o\" notation** \"$o(\\delta x)$,\" which denotes any function whose magnitude shrinks much faster than $|\\delta x|$ as $\\delta x\\rightarrow0$ so that for sufficiently small $\\delta x$ it is negligible compared to the linear $f^{\\prime}(x)\\delta x$ term. (Variants of this notation are commonly used in computer science, and there is a formal definition that we omit here.¹) Examples of such higher-order terms include $(\\delta x)^{2}$, $(\\delta x)^{3}$, $(\\delta x)^{1.001}$, and $(\\delta x)/log(\\delta x)$\n",
    "\n",
    "**Remark 6.** Here, $\\delta x$ is not an infinitesimal but rather a small number. Note that our symbol \"$\\delta$\" (a Greek lowercase \"delta\") is not the same as the symbol \"$\\partial$\" commonly used to denote partial derivatives. This notion of a derivative may remind you of the first two terms in a Taylor series $f(x+\\delta x)=f(x)+f^{\\prime}(x)\\delta x+\\cdot\\cdot\\cdot$ (though in fact it is much more basic than Taylor series!), and the notation will generalize nicely to higher dimensions Briefly, a function $g(\\delta x)$ is $o(\\delta x)$ if $lim_{\\delta x \\to 0} \\frac{g(\\delta x)}{\\delta x} = 0$. We will return to this subject in Section 5.2.\n",
    "\n",
    "and other vector spaces. In differential notation, we can express the same idea as:\n",
    "\n",
    "$df=f(x+dx)-f(x) \\approx f^{\\prime}(x)dx.$\n",
    "\n",
    "In this notation we implicitly drop the $o(\\delta x)$ term that vanishes in the limit as $dx$ becomes infinitesimally small. We will use this as the more generalized definition of a derivative. In this formulation, we avoid dividing by $dx$, because soon we will allow x (and hence $dx$) to be something other than a number; if $dx$ is a vector, we won't be able to divide by it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear operators\n",
    "\n",
    "From the perspective of linear algebra, given a function f, we consider the derivative of f, to be the **linear operator** $f^{\\prime}(x)$ such that\n",
    "\n",
    "$df=f(x+dx)-f(x) \\approx f^{\\prime}(x)[dx]$.\n",
    "\n",
    "As above, you should think of the differential notation $dx$ as representing an arbitrary small change in x, where we are implicitly dropping any $o(dx)$ terms, i.e. terms that decay faster than linearly as $dx\\rightarrow0$. Often, we will omit the square brackets and write simply $f^{\\prime}(x)dx$ instead of $f^{\\prime}(x)[dx]$, but this should be understood as the linear operator $f^{\\prime}(x)$ acting on $dx$; don't write $dx f^{\\prime}(x)$ which will generally be nonsense! This definition will allow us to extend differentiation to arbitrary vector spaces of inputs and outputs $f(x)$. (More technically, we will require vector spaces with a norm $||x||$, called \"Banach spaces,\" in order to precisely define the $o(\\delta x)$ terms that are dropped. We will come back to the subject of Banach spaces later.)\n",
    "\n",
    "**Recall 7 (Vector Space)**\n",
    "\n",
    "Loosely, a **vector space** (over $\\mathbb{R}$) is a set of elements in which addition and subtraction between elements is defined, along with multiplication by real scalars. For instance, while it does not make sense to multiply arbitrary vectors in $\\mathbb{R}^{n}$, we can certainly add them together, and we can certainly scale the vectors by a constant factor. Some examples of vector spaces include:\n",
    "\n",
    "* $\\mathbb{R}^{n}$, the set of all n-dimensional vectors, as described in the above.\n",
    "* More generally, $\\mathbb{R}^{n\\times m}$, the space of $n\\times m$ matrices with real entries. Notice again that, if $n\\ne m$, then multiplication between elements is not defined.\n",
    "* $C^{0}(\\mathbb{R}^{n})$, the set of continuous functions over $\\mathbb{R}^{n}$, with addition defined pointwise.\n",
    "\n",
    "**Recall 8 (Linear Operator)**\n",
    "\n",
    "Recall that a **linear operator** is a map L from a vector v in vector space V to a vector $L[v]$ (sometimes denoted simply Lv) in some other vector space. Specifically, L is linear if for any vectors $v_1, v_2$ and any scalar $\\alpha \\in \\mathbb{R}$:\n",
    "\n",
    "1.  $L[v_{1}+v_{2}]=L[v_{1}]+L[v_{2}]$\n",
    "2.  $L[\\alpha v]=\\alpha L[v]$\n",
    "\n",
    "**Remark:** In this course, $f^{\\prime}$ is a map that takes in an x and spits out a linear operator $f^{\\prime}(x)$ (the derivative of f at x). Furthermore, $f^{\\prime}(x)$ is a linear map that takes in an input direction v and gives an output vector $f^{\\prime}(x)[v]$ (which we will later interpret as a directional derivative, see Sec. 2.2.1). When the direction v is an infinitesimal dx, the output $f^{\\prime}(x)[dx]=df$ is the differential of f (the corresponding infinitesimal change in f).\n",
    "\n",
    "Some examples of linear operators include Multiplication by scalars $\\alpha$, i.e. $L[v]=\\alpha v.$ Also multiplication of column vectors v by matrices A, i.e. $L[v]=Av$.\n",
    "* Some functions like $f(x)=x^{2}$ are obviously nonlinear. But what about $f(x)=x+1?$ This may look linear if you plot it, but it is not a linear operation, because $f(2x)=2x+1\\ne2f(x)$; such functions, which are linear plus a nonzero constant, are known as **affine**.\n",
    "* There are also many other examples of linear operations that are not so convenient or easy to write down as matrix-vector products. For example, if A is a $3\\times3$ matrix, then $L[A]=AB+CA$ is a linear operator given $3\\times3$ matrices B, C. The transpose $f(x)=x^{T}$ of a column vector x is linear, but is not given by any matrix multiplied by x. Or, if we consider vector spaces of functions, then the calculus operations of differentiation and integration are linear operators too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Directional derivatives\n",
    "\n",
    "There is an equivalent way to interpret this linear-operator viewpoint of a derivative, which you may have seen before in multivariable calculus: as a **directional derivative**. If we have a function $f(x)$ of arbitrary vectors x, then the directional derivative at x in a direction (vector) v is defined as:\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\alpha}f(x+\\alpha v)|_{\\alpha=0}=lim_{\\delta\\alpha\\rightarrow0}\\frac{f(x+\\delta\\alpha~v)-f(x)}{\\delta\\alpha}$ (1)\n",
    "\n",
    "where $\\alpha$ is a scalar. This transforms derivatives back into single-variable calculus from arbitrary vector spaces. It measures the rate of change of f in the direction v from x. But it turns out that this has a very simple relationship to our linear operator $f^{\\prime}(x)$ from above, because (dropping higher-order terms due to the limit $\\delta\\alpha\\rightarrow0$):\n",
    "\n",
    "$f(x+\\underbrace{\\delta\\alpha~v}_{dx}})-f(x) \\approx f^{\\prime}(x)[dx]=f'(x)[\\delta \\alpha v] = \\delta\\alpha~f^{\\prime}(x)[v]$\n",
    "\n",
    "where we have factored out the scalar $\\delta\\alpha$ in the last step thanks to $f^{\\prime}(x)$ being a linear operator. Comparing with the definition above, we immediately find that the directional derivative is:\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\alpha}f(x+\\alpha v)|_{\\alpha=0}=f^{\\prime}(x)[v]$ (2)\n",
    "\n",
    "It is exactly equivalent to our $f^{\\prime}(x)$ from before! (We can also see this as an instance of the chain rule from Sec. 2.5.) One lesson from this viewpoint is that it is perfectly reasonable to input an arbitrary non-infinitesimal vector v into $f^{\\prime}(x)[v]$; the result is not a $df$, but is simply a directional derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Revisiting multivariable calculus, Part 1: Scalar-valued functions\n",
    "\n",
    "Let f be a scalar-valued function, which takes in \"column\" vectors $x\\in\\mathbb{R}^{n}$ and produces a scalar (in R). Then,\n",
    "$df = f(x+dx)-f(x) \\approx f'(x) [dx] = \\text{scalar}.$\n",
    "\n",
    "Therefore, since $dx$ is a column vector (in an arbitrary direction, representing an arbitrary small change in x), the linear operator $f^{\\prime}(x)$ that produces a scalar $df$ must be a **row vector** (a \"1-row matrix\", or more formally something called a covector or \"dual\" vector or \"linear form\")! We call this row vector the transpose of the **gradient**:\n",
    "\n",
    "$(\\nabla f)^{T}$, so that $df$ is the dot (\"inner\") product of $dx$ with the gradient. So we have that\n",
    "\n",
    "$df=\\nabla f\\cdot dx=\\underbrace{(\\nabla f)^{T}}_{f^{\\prime}(x)}dx$\n",
    "\n",
    "where\n",
    "\n",
    "$dx=\\begin{pmatrix}dx_{1}\\\\ dx_{2}\\\\ \\vdots\\\\ dx_{n}\\end{pmatrix}.$\n",
    "\n",
    "Some authors view the gradient as a row vector (equating it with $f^{\\prime}$ or the Jacobian), but treating it as a \"column vector\" (the transpose of $f^{\\prime}$), as we do in this course, is a common and useful choice. As a column vector, the gradient can be viewed as the \"uphill\" (steepest-ascent) direction in the space, as depicted in Fig. 2. Furthermore, it is also easier to generalize to scalar functions of other vector spaces. In any case, for this class, we will always define $\\nabla f$ to have the same \"shape\" as x, so that $df$ is a dot product (\"inner product\") of $dx$ with the gradient.\n",
    "\n",
    "This is perfectly consistent with the viewpoint of the gradient that you may remember from multivariable calculus, in which the gradient was a vector of components\n",
    "\n",
    "$\\nabla f=\\begin{pmatrix}\\frac{\\partial f}{\\partial x_{1}}\\\\ \\frac{\\partial f}{\\partial x_{2}}\\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial x_{n}}\\end{pmatrix};$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$df=f(x+dx)-f(x)\\approx \\nabla f\\cdot dx=\\frac{\\partial f}{\\partial x_{1}}dx_{1}+\\frac{\\partial f}{\\partial x_{2}}dx_{2}+\\cdot\\cdot\\cdot+\\frac{\\partial f}{\\partial x_{n}}dx_{n}$ .\n",
    "\n",
    "While a component-wise viewpoint may sometimes be convenient, we want to encourage you to view the vector x as a whole, not simply a collection of components, and to learn that it is often more convenient and elegant to differentiate expressions without taking the derivative component-by-component, a new approach that will generalize better to more complicated inputs/output vector spaces. Let's look at an example to see how we compute this differential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 10**\n",
    "\n",
    "Consider $f(x)=x^{T}Ax$ where $x\\in\\mathbb{R}^{n}$ and A is a square $n\\times n$ matrix, and thus $f(x)\\in\\mathbb{R}$. Compute $df$, $f^{\\prime}(x)$, and $\\nabla f$.\n",
    "\n",
    "We can do this directly from the definition.\n",
    "$df=f(x+dx)-f(x)$\n",
    "$=(x+dx)^{T}A(x+dx)-x^{T}Ax$\n",
    "$= (x^T + dx^T)A(x+dx) - x^TAx$\n",
    "$= (x^T + dx^T)(Ax + Adx) - x^TAx$\n",
    "$=x^{T}Ax+x^{T}Adx+dx^{T}Ax+dx^{T}Adx-x^{T}Ax$\n",
    "\n",
    "Here, we dropped terms with more than one $dx$ factor (like $dx^TAdx$) as these are asymptotically negligible (higher order). We are left with $df \\approx x^TAdx + dx^TAx$.\n",
    "\n",
    "Another trick was to combine $dx^{T}Ax$ and $x^{T}Adx$ by realizing that these are scalars (1x1 matrices) and hence equal to their own transpose: $dx^{T}Ax=(dx^{T}Ax)^{T}=x^{T}A^{T}dx$. So we have:\n",
    "\n",
    "$df \\approx x^TAdx + x^TA^Tdx = (x^TA + x^TA^T)dx = x^T(A+A^T)dx$.\n",
    "\n",
    "From this, we can identify the derivative and the gradient:\n",
    "$df = \\underbrace{x^{T}(A+A^{T})}_{f^{\\prime}(x)=(\\nabla f)^{T}}dx\\Rightarrow\\nabla f=(A+A^{T})x.$\n",
    "\n",
    "Hence, we have found that $f^{\\prime}(x)=x^{T}(A+A^{T})=(\\nabla f)^{T},$ or equivalently $\\nabla f=[x^{T}(A+A^{T})]^{T}=(A+A^{T})x$. It is, of course, also possible to compute the same gradient component-by-component, the way you probably learned to do in multivariable calculus. First, you would need to write $f(x)$ explicitly in terms of the components of x, as $f(x)=x^{T}Ax=\\sum_{i,j}x_{i}A_{i,j}x_{j}$. Then, you would compute $\\partial f/\\partial x_{k}$ for each k, taking care that $x_k$ appears twice in the f summation. However, this approach is awkward, error-prone, labor-intensive, and quickly becomes worse as we move on to more complicated functions. It is much better, we feel, to get used to treating vectors and matrices as a whole, not as mere collections of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Revisiting multivariable calculus, Part 2: Vector-valued functions\n",
    "\n",
    "Next time, we will revisit multi-variable calculus (18.02 at MIT) again in a Part 2, where now f will be a vector-valued function, taking in vectors $x\\in\\mathbb{R}^{n}$ and giving vector outputs $f(x)\\in\\mathbb{R}^{m}$. Then, $df$ will be a m-component column vector, $dx$ will be an n-component column vector, and we must get a linear operator $f^{\\prime}(x)$ satisfying\n",
    "\n",
    "$\\underbrace{df}_{m \\text{ components}}=\\underbrace{f^{\\prime}(x)}_{m\\times n} \\underbrace{dx}_{n \\text{ components}}$\n",
    "\n",
    "so $f^{\\prime}(x)$ must be an $m\\times n$ matrix called the **Jacobian** of f! The Jacobian matrix J represents the linear operator that takes dx to df:\n",
    "\n",
    "$df=Jdx$.\n",
    "\n",
    "The matrix J has entries $J_{ij}=\\frac{\\partial f_{i}}{\\partial x_{j}}$ (corresponding to the i-th row and the j-th column of J). So now, suppose that $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}^{2}$. Let's understand how we would compute the differential of f:\n",
    "\n",
    "$df=\\begin{pmatrix}\\frac{\\partial f_{1}}{\\partial x_{1}}&\\frac{\\partial f_{1}}{\\partial x_{2}}\\\\ \\frac{\\partial f_{2}}{\\partial x_{1}}&\\frac{\\partial f_{2}}{\\partial x_{2}}\\\\ \\end{pmatrix}\\begin{pmatrix}dx_{1}\\\\ dx_{2}\\end{pmatrix}=\\begin{pmatrix}\\frac{\\partial f_{1}}{\\partial x_{1}}dx_{1}+\\frac{\\partial f_{1}}{\\partial x_{2}}dx_{2}\\\\ \\frac{\\partial f_{2}}{\\partial x_{1}}dx_{1}+\\frac{\\partial f_{2}}{\\partial x_{2}}dx_{2}\\\\ \\end{pmatrix}$\n",
    "\n",
    "Let's compute an example.\n",
    "\n",
    "**Example 11**\n",
    "\n",
    "Consider the function $f(x)=Ax$ where A is a constant $m\\times n$ matrix. Then, by applying the distributive law for matrix-vector products, we have\n",
    "\n",
    "$df=f(x+dx)-f(x)=A(x+dx)-Ax$\n",
    "$=Ax+Adx-Ax=Adx=f^{\\prime}(x)dx.$\n",
    "\n",
    "Therefore, $f^{\\prime}(x)=A$\n",
    "\n",
    "Notice then that the linear operator A is its own Jacobian matrix! Let's now consider some derivative rules.\n",
    "\n",
    "* **Sum Rule:** Given $f(x)=g(x)+h(x)$ we get that\n",
    "\n",
    "    $df=dg+dh\\Rightarrow f^{\\prime}(x)dx=g^{\\prime}(x)dx+h^{\\prime}(x)dx.$\n",
    "\n",
    "    Hence, $f^{\\prime}=g^{\\prime}+h^{\\prime}$ as we should expect. This is the linear operator $f^{\\prime}(x)[v]=g^{\\prime}(x)[v]+h^{\\prime}(x)[v]$, and note that we can sum linear operators (like $g^{\\prime}$ and $h^{\\prime})$ just like we can sum matrices! In this way, linear operators form a vector space.\n",
    "\n",
    "* **Product Rule:** Suppose $f(x)=g(x)h(x).$ Then,\n",
    "\n",
    "    $df=f(x+dx)-f(x)$\n",
    "    $=g(x+dx)h(x+dx)-g(x)h(x)$\n",
    "    $=(\\underbrace{g(x)+g^{\\prime}(x)dx}_{g+dg})(\\underbrace{h(x)+h^{\\prime}(x)dx}_{h+dh})-g(x)h(x)$\n",
    "    $=(g+dg)(h+dh) - gh$\n",
    "    $=gh+g(dh)+(dg)h+(dg)(dh)-gh$\n",
    "    $=g(dh)+(dg)h$,\n",
    "\n",
    "    where the $(dg)(dh)$ term is higher-order and hence dropped in infinitesimal notation. Note, as usual, that $g(dh)$ and $(dg)h$ may not commute now as they may no longer be scalars! Let's look at some short examples of how we can apply the product rule nicely.\n",
    "\n",
    "**Example 12**\n",
    "\n",
    "Let $f(x)=Ax$ (mapping $\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ where A is a constant $m\\times n$ matrix. Then,\n",
    "\n",
    "$df=d(Ax)=(dA)x+A(dx)=0 \\cdot x + Adx = Adx\\Rightarrow f^{\\prime}(x)=A.$\n",
    "\n",
    "We have $dA=0$ here because A is a constant matrix and does not change when we change x.\n",
    "\n",
    "**Example 13**\n",
    "\n",
    "Let $f(x)=x^{T}Ax$ (mapping $\\mathbb{R}^{n}\\rightarrow\\mathbb{R})$. Then, applying the product rule to $x^T(Ax)$:\n",
    "\n",
    "$df=d(x^T)(Ax)+x^{T}d(Ax)$\n",
    "$= (dx^T)(Ax) + x^T(Adx)$\n",
    "As we saw before, since $df$ is a scalar, we can transpose the first term: $dx^T(Ax) = (dx^T Ax)^T = x^T A^T dx$.\n",
    "So, $df = x^T A^T dx + x^T Adx = x^T(A^T+A)dx = x^{T}(A+A^{T})dx=(\\nabla f)^{T}dx$,\n",
    "\n",
    "and hence $f^{\\prime}(x)=x^{T}(A+A^{T})$. (In the common case where A is symmetric, this simplifies to $f^{\\prime}(x)=2x^{T}A.)$\n",
    "Note that here we have applied Example 12 in computing $d(Ax)=Adx$ Furthermore, f is a scalar valued function, so we may also obtain the gradient $\\nabla f=(A+A^{T})x$ as before (which simplifies to $2Ax$ if A is symmetric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 The Chain Rule\n",
    "\n",
    "One of the most important rules from differential calculus is the **chain rule**, because it allows us to differentiate complicated functions built out of compositions of simpler functions. This chain rule can also be generalized to our differential notation in order to work for functions on arbitrary vector spaces:\n",
    "\n",
    "**Chain Rule:** Let $f(x)=g(h(x))$. Then,\n",
    "\n",
    "$df=f(x+dx)-f(x)=g(h(x+dx))-g(h(x))$\n",
    "Let's look at the argument of $g$. Inside, we have $h(x+dx)$, which is approximately $h(x) + h'(x)[dx]$. Let's call $y=h(x)$ and $dy=h'(x)[dx]$.\n",
    "So, we have $g(y+dy)$. The differential of $g$ at $y$ is $g'(y)[dy]$.\n",
    "Substituting back in for $y$ and $dy$:\n",
    "$df \\approx g'(h(x))[h'(x)[dx]]$\n",
    "$=g^{\\prime}(h(x))h^{\\prime}(x)[dx]$\n",
    "\n",
    "where $g^{\\prime}(h(x))h^{\\prime}(x)$ is a composition of linear operators, which for vector functions corresponds to matrix multiplication. In other words, $f^{\\prime}(x)=g^{\\prime}(h(x))h^{\\prime}(x)$: the Jacobian (linear operator) $f^{\\prime}$ is simply the product (composition) of the Jacobians, g'h'. Ordering matters because linear operators (and matrices) do not generally commute: the product must be evaluated from outputs to inputs (left-to-right in the formula).\n",
    "\n",
    "Let's look more carefully at the shapes of these Jacobian matrices in an example where each function maps a column vector to a column vector:\n",
    "\n",
    "**Example 15**\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{n}$, $h(x)\\in\\mathbb{R}^{p}$, and $g(y)\\in\\mathbb{R}^{m}$ where $y=h(x)$. Then, let $f(x)=g(h(x))$ mapping from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$. The chain rule then states that\n",
    "\n",
    "$\\underbrace{f^{\\prime}(x)}_{m \\times n}=\\underbrace{g^{\\prime}(h(x))}_{m \\times p} \\underbrace{h^{\\prime}(x)}_{p \\times n}$,\n",
    "\n",
    "which makes sense as $g^{\\prime}$ is an $m\\times p$ matrix and $h^{\\prime}$ is a $p\\times n$ matrix, so that the product gives an $m\\times n$ matrix $f^{\\prime}!$ However, notice that this is not the same as $h^{\\prime}(x)g^{\\prime}(h(x))$ as you cannot (if $n\\ne m$) multiply a $p\\times n$ matrix and an $m\\times p$ matrix together in that order, and even if $n=m$ you will get the wrong answer since they probably won't commute.\n",
    "\n",
    "Not only does the order of the multiplication matter, but the **associativity** of matrix multiplication matters practically. Let's consider a function\n",
    "\n",
    "$f(x)=a(b(c(x)))$\n",
    "\n",
    "where $c:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$, $b:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{q}$, and $a:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}^{m}$. Then, we have that, by the chain rule,\n",
    "\n",
    "$f^{\\prime}(x)=a^{\\prime}(b(c(x))) \\cdot b^{\\prime}(c(x)) \\cdot c^{\\prime}(x)$\n",
    "Notice that this is the same as\n",
    "\n",
    "$f^{\\prime}=(a^{\\prime}b^{\\prime})c^{\\prime}=a^{\\prime}(b^{\\prime}c^{\\prime})$\n",
    "\n",
    "by associativity (omitting the function arguments for brevity). The left-hand side is multiplication from left to right, and the right-hand side is multiplication from right to left. But who cares? Well it turns out that associativity is deeply important. So important that the two orderings have names: multiplying left-to-right is called **\"reverse mode\"** and multiplying right-to-left is called **\"forward mode\"** in the field of automatic differentiation (AD). Reverse-mode differentation is also known as an \"adjoint method\" or \"backpropagation\" in some contexts, which we will explore in more detail later. Why does this matter? Let's think about the computational cost of matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Cost of Matrix Multiplication\n",
    "\n",
    "If you multiply a $m\\times q$ matrix by a $q\\times p$ matrix, you normally do it by computing $mp$ dot products of length $q$ (or some equivalent re-ordering of these operations). To do a dot product of length $q$ requires $q$ multiplications and $q-1$ additions of scalars. Overall, this is approximately $2mpq$ scalar operations in total. In computer science, you would write that this is $`\\Theta(mpq)\":$ the computational effort is asymptotically proportional to $mpq$ for large m, p, q.\n",
    "\n",
    "So why does the order of the chain rule matter? Consider the following two examples.\n",
    "\n",
    "**Example 16**\n",
    "\n",
    "Suppose you have a lot of inputs $n\\gg1$, and only one output $m=1,$ with lots of intermediate values, i.e. $q=p=n$. This means the Jacobians have sizes: $a'$ is $1 \\times n$, $b'$ is $n \\times n$, and $c'$ is $n \\times n$.\n",
    "The derivative is $f' = a' b' c'$.\n",
    "* **Reverse Mode (left-to-right):** $(a'b')c'$.\n",
    "    1.  $a'b'$: $(1 \\times n) \\cdot (n \\times n)$ gives a $1 \\times n$ matrix. Cost is $\\Theta(1 \\cdot n \\cdot n) = \\Theta(n^2)$.\n",
    "    2.  (result)$c'$: $(1 \\times n) \\cdot (n \\times n)$ gives a $1 \\times n$ matrix. Cost is $\\Theta(1 \\cdot n \\cdot n) = \\Theta(n^2)$.\n",
    "    Total cost: $\\Theta(n^2)$.\n",
    "* **Forward Mode (right-to-left):** $a'(b'c')$.\n",
    "    1.  $b'c'$: $(n \\times n) \\cdot (n \\times n)$ gives an $n \\times n$ matrix. Cost is $\\Theta(n \\cdot n \\cdot n) = \\Theta(n^3)$.\n",
    "    2.  $a'$(result): $(1 \\times n) \\cdot (n \\times n)$ gives a $1 \\times n$ matrix. Cost is $\\Theta(1 \\cdot n \\cdot n) = \\Theta(n^2)$.\n",
    "    Total cost: $\\Theta(n^3)$.\n",
    "\n",
    "Then reverse mode (left-to-right) will cost $\\Theta(n^{2})$ scalar operations while forward mode (right-to-left) would cost $\\Theta(n^{3})$ This is a huge cost difference, depicted schematically in Fig. 3.\n",
    "\n",
    "Conversely, suppose you have a lot of outputs $m\\gg1$ and only one input $n=1$, with lots of intermediate values $q=p=m$. Then reverse mode would cost $\\Theta(m^{3})$ operations but forward mode would be only $\\Theta(m^{2})!$\n",
    "\n",
    "**Moral:** If you have a lot of inputs and few outputs (the usual case in machine learning and optimization), compute the chain rule left-to-right (**reverse mode**). If you have a lot of outputs and few inputs, compute the chain rule right-to-left (**forward mode**). We return to this in Sec. 8.4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
